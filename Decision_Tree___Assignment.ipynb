{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI7J4wy15za7"
      },
      "outputs": [],
      "source": [
        "# Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "# Ans 1: A Decision Tree is a supervised machine learning algorithm that works like a flowchart.\n",
        "# It's a structure used to make decisions by splitting a dataset into smaller and smaller subsets based\n",
        "# on its features, ultimately leading to a final outcome or prediction.\n",
        "# New Data Point: Outlook = Sunny, Humidity = High, Wind = Weak\n",
        "# Start at Root Node: The node asks, \"Outlook?\"\n",
        "# The data's value is \"Sunny,\" so it follows the \"Sunny\" branch.\n",
        "# Arrive at New Node: This node asks, \"Humidity?\"\n",
        "# The data's value is \"High,\" so it follows the \"High\" branch.\n",
        "# Arrive at Leaf Node: This is a leaf node that says \"Class: No\"\n",
        "\n",
        "# Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "# Ans 2: Gini Impurity and Entropy are both metrics used to measure the \"impurity\" or \"disorder\" of a set of data.\n",
        "# In a decision tree, \"impurity\" refers to how mixed the classes are at a particular node.\n",
        "# A \"pure\" node is ideal: all data points in it belong to a single class (e.g., 100% \"Spam\").\n",
        "# An \"impure\" node is the worst case: the data points are split evenly among all classes (e.g., 50% \"Spam,\" 50% \"Not Spam\")\n",
        "# Gini Impurity\n",
        "# Gini Impurity measures the likelihood of a randomly chosen element from a node being incorrectly classified if it were randomly\n",
        "#  labeled according to the distribution of classes in that node.\n",
        "# Entropy\n",
        "# Entropy is a concept from information theory that measures the amount of uncertainty or randomness (i.e., disorder) in a node.\n",
        "\n",
        "# Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "# Ans 3: Pruning means reducing the size of a decision tree by removing unnecessary branches or nodes that add little predictive power.\n",
        "# It helps the model generalize better to unseen data.\n",
        "# Post-pruning allows the tree to grow completely first, and then removes branches that don’t improve performance on validation data.\n",
        "# Pre-pruning stops the tree from growing too deep by setting constraints before the tree is fully built.\n",
        "# advantage:\n",
        "# Better generalization — It helps the model keep only the most meaningful splits, improving performance on unseen data.\n",
        "\n",
        "# Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "# Ans 4: Information Gain (IG) is a measure of how much “information” or “purity” is gained by splitting a dataset based on a particular feature.\n",
        "# It tells us how good a feature is at separating the classes (like YES/NO, spam/not spam, etc.).\n",
        "# The higher the Information Gain, the better the feature is for making a split.\n",
        "\n",
        "# Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "# Ans 5: Decision Trees are widely used in finance, healthcare, marketing, and business decision-making. Banks use them for loan\n",
        "#  approvals and fraud detection, while in healthcare, they assist in disease diagnosis and treatment prediction. Marketers apply\n",
        "#  them for customer segmentation and churn prediction. Their main advantages include easy interpretation, handling both numerical and\n",
        "#  categorical data, and minimal preprocessing. However, Decision Trees often overfit, are sensitive to small data changes, and may bias\n",
        "#   features with many categories. Despite these drawbacks, they remain a popular choice for interpretable and efficient decision-making,\n",
        "#   especially when combined with ensemble methods like Random Forests.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", round(accuracy * 100, 2), \"%\")\n",
        "\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NcaGIhz8SRP",
        "outputId": "bf965fc6-0562-4071-8d8c-493e072737db"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 100.0 %\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "# a fully-grown tree.\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf_limited = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "\n",
        "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "\n",
        "print(\"Accuracy with max_depth=3:\", round(accuracy_limited * 100, 2), \"%\")\n",
        "print(\"Accuracy with fully-grown tree:\", round(accuracy_full * 100, 2), \"%\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJqTFiBX8oMt",
        "outputId": "d5c203cd-d9a9-4149-d602-293732a04a4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 100.0 %\n",
            "Accuracy with fully-grown tree: 100.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Load the Boston Housing Dataset\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor(criterion='squared_error', random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", round(mse, 2))\n",
        "\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature, importance in zip(X.columns, regressor.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyQMk1C083ly",
        "outputId": "6d940e2d-9c01-4c72-ad51-4edfbb101291"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 11.59\n",
            "\n",
            "Feature Importances:\n",
            "CRIM: 0.0585\n",
            "ZN: 0.0010\n",
            "INDUS: 0.0099\n",
            "CHAS: 0.0003\n",
            "NOX: 0.0071\n",
            "RM: 0.5758\n",
            "AGE: 0.0072\n",
            "DIS: 0.1096\n",
            "RAD: 0.0016\n",
            "TAX: 0.0022\n",
            "PTRATIO: 0.0250\n",
            "B: 0.0119\n",
            "LSTAT: 0.1900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "# GridSearchCV\n",
        "# ● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6, None],\n",
        "    'min_samples_split': [2, 3, 4, 5, 10]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=clf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Model Accuracy after tuning:\", round(accuracy * 100, 2), \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_IOMmgf9Nvy",
        "outputId": "5ce577ff-e98a-4fe4-f3a8-6b6c3553f938"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy after tuning: 100.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "# wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "# mixed data types and some missing values.\n",
        "# Explain the step-by-step process you would follow to:\n",
        "# ● Handle the missing values\n",
        "# ● Encode the categorical features\n",
        "# ● Train a Decision Tree model\n",
        "# ● Tune its hyperparameters\n",
        "# ● Evaluate its performance\n",
        "# And describe what business value this model could provide in the real-world\n",
        "# setting\n",
        "\n",
        "# Ans 10:\n",
        "# 1. Handle Missing Values:\n",
        "# Remove irrelevant columns, impute numerical with mean/median, categorical with mode or “Unknown.” Add missing indicators to help model recognize data gaps.\n",
        "# 2. Encode Categorical Features:\n",
        "# Use one-hot for nominal, label for ordinal features. Merge rare categories. Apply encoders in pipelines to prevent data leakage and ensure consistency.\n",
        "# 3. Train Decision Tree Model:\n",
        "# Split data, train DecisionTreeClassifier, handle imbalance using class weights, and evaluate performance using accuracy, precision, recall, and F1-score metrics for validation.\n",
        "# 4. Tune Hyperparameters:\n",
        "# Use GridSearchCV to optimize max_depth, min_samples_split, and criterion. Apply cross-validation to prevent overfitting and ensure best generalization performance.\n",
        "# 5. Evaluate & Business Value:\n",
        "# Evaluate with accuracy, recall, F1-score, ROC-AUC. Enables early disease detection, reduces costs, and supports better patient decisions through data-driven healthcare insights."
      ],
      "metadata": {
        "id": "AJm21M5j9f-F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}